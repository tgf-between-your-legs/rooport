# ROOPORT Testing and Validation Guide

## ðŸ§ª Comprehensive Testing Framework

This guide provides systematic testing procedures to validate your ROOPORT installation and understand what functionality is currently operational vs. theoretical.

## ðŸ“‹ Testing Hierarchy

### Level 1: Installation Verification
### Level 2: Component Integration Testing  
### Level 3: AI Engine Functionality Testing
### Level 4: Real-World Workflow Testing
### Level 5: Performance and Learning Validation

---

## ðŸ”§ Level 1: Installation Verification

### 1.1 Basic Package Installation

```bash
# Test 1: Verify ROOPORT CLI is installed
rooport --version
# Expected: "ROOPORT 1.0.0" or similar

# Test 2: Check help system
rooport --help
# Expected: Comprehensive CLI help with all options

# Test 3: Test workspace detection
rooport --workspace . --verbose
# Expected: Workspace path detection and AI engine initialization messages
```

### 1.2 Configuration Validation

```bash
# Test configuration loading
python -c "
from rooport.config.autonomous_mode import get_autonomous_mode
enabled, level = get_autonomous_mode()
print(f'Autonomous mode: {\"enabled\" if enabled else \"disabled\"}, level: {level}')
"
# Expected: Configuration loads without errors
```

### 1.3 Dependencies Check

```python
# Create test script: test_dependencies.py
import sys

def test_imports():
    try:
        from rooport.core.orchestrator import UltimateAgenticOrchestrator
        print("âœ… Ultimate Agentic Orchestrator import successful")
    except ImportError as e:
        print(f"âŒ Orchestrator import failed: {e}")
    
    try:
        from rooport.core.rag_engine import AgenticRAGEngine
        print("âœ… Agentic RAG Engine import successful")
    except ImportError as e:
        print(f"âŒ RAG Engine import failed: {e}")
    
    try:
        from rooport.core.learning_system import ContinuousLearningSystem
        print("âœ… Continuous Learning System import successful")
    except ImportError as e:
        print(f"âŒ Learning System import failed: {e}")
    
    try:
        from rooport.core.proactive_engine import ProactiveOrchestrationEngine
        print("âœ… Proactive Orchestration Engine import successful")
    except ImportError as e:
        print(f"âŒ Proactive Engine import failed: {e}")

if __name__ == "__main__":
    test_imports()
```

```bash
python test_dependencies.py
# Expected: All imports successful
```

---

## ðŸ”— Level 2: Component Integration Testing

### 2.1 ConPort MCP Connection Test

```python
# Create test script: test_conport_connection.py
import asyncio
import json
from pathlib import Path

async def test_conport_mcp():
    """Test ConPort MCP server connectivity"""
    print("ðŸ§ª Testing ConPort MCP Connection...")
    
    # Test 1: Check if ConPort database can be created
    workspace_path = Path.cwd()
    context_portal_path = workspace_path / "context_portal"
    context_portal_path.mkdir(exist_ok=True)
    
    print(f"ðŸ“ Workspace: {workspace_path}")
    print(f"ðŸ“ ConPort directory: {context_portal_path}")
    
    # Test 2: Simulate ConPort MCP tool call
    try:
        # This would normally go through MCP, but we'll test the concept
        test_data = {
            "workspace_id": str(workspace_path),
            "product_context": {
                "name": "ROOPORT Test Project",
                "description": "Testing ConPort integration",
                "created_at": "2025-05-24T19:48:00Z"
            }
        }
        
        # Write test data to verify file operations work
        test_file = context_portal_path / "test_data.json"
        with open(test_file, 'w') as f:
            json.dump(test_data, f, indent=2)
        
        print("âœ… ConPort file operations working")
        
        # Read back test data
        with open(test_file, 'r') as f:
            loaded_data = json.load(f)
        
        assert loaded_data["product_context"]["name"] == "ROOPORT Test Project"
        print("âœ… ConPort data persistence working")
        
        # Cleanup
        test_file.unlink()
        
    except Exception as e:
        print(f"âŒ ConPort test failed: {e}")
        return False
    
    return True

# Run test
if __name__ == "__main__":
    result = asyncio.run(test_conport_mcp())
    print(f"ConPort integration test: {'âœ… PASSED' if result else 'âŒ FAILED'}")
```

### 2.2 Autonomous Mode Configuration Test

```python
# Create test script: test_autonomous_mode.py
from rooport.config.autonomous_mode import (
    get_autonomous_mode, 
    set_autonomous_mode, 
    should_execute_autonomous_action,
    get_status_display
)

def test_autonomous_mode_system():
    """Test autonomous mode configuration system"""
    print("ðŸ§ª Testing Autonomous Mode System...")
    
    # Test 1: Default state
    enabled, level = get_autonomous_mode()
    print(f"Default state - Enabled: {enabled}, Level: {level}")
    
    # Test 2: Set different modes
    test_modes = ["off", "minimal", "standard", "full"]
    
    for mode in test_modes:
        try:
            set_autonomous_mode(mode)
            enabled, level = get_autonomous_mode()
            status = get_status_display()
            
            print(f"Mode '{mode}' - Status: {status}")
            
            # Test confidence thresholds
            if mode != "off":
                high_confidence = should_execute_autonomous_action(0.9, "suggestion")
                low_confidence = should_execute_autonomous_action(0.3, "suggestion")
                
                print(f"  High confidence (0.9): {'Execute' if high_confidence else 'Ask user'}")
                print(f"  Low confidence (0.3): {'Execute' if low_confidence else 'Ask user'}")
            
        except Exception as e:
            print(f"âŒ Error testing mode '{mode}': {e}")
            return False
    
    # Reset to default
    set_autonomous_mode("off")
    print("âœ… Autonomous mode system test completed")
    return True

if __name__ == "__main__":
    result = test_autonomous_mode_system()
    print(f"Autonomous mode test: {'âœ… PASSED' if result else 'âŒ FAILED'}")
```

---

## ðŸ¤– Level 3: AI Engine Functionality Testing

### 3.1 Ultimate Agentic Orchestrator Test

```python
# Create test script: test_orchestrator.py
import asyncio
from datetime import datetime

# Mock ConPort client for testing
class MockConPortClient:
    def __init__(self):
        self.data = {}
    
    async def log_custom_data(self, workspace_id, category, key, value):
        if category not in self.data:
            self.data[category] = {}
        self.data[category][key] = value
        return {"status": "success", "id": f"{category}-{key}"}
    
    async def get_product_context(self, workspace_id):
        return {
            "name": "Test Project",
            "description": "ROOPORT functionality test",
            "tech_stack": ["Python", "AI", "ConPort"]
        }
    
    async def get_recent_activity_summary(self, workspace_id, **kwargs):
        return {
            "decisions": [{"summary": "Use Python for backend", "timestamp": datetime.now().isoformat()}],
            "progress": [{"description": "Setup testing framework", "status": "IN_PROGRESS"}]
        }

async def test_orchestrator():
    """Test Ultimate Agentic Orchestrator functionality"""
    print("ðŸ§ª Testing Ultimate Agentic Orchestrator...")
    
    try:
        from rooport.core.orchestrator import UltimateAgenticOrchestrator
        
        # Initialize with mock client
        mock_conport = MockConPortClient()
        orchestrator = UltimateAgenticOrchestrator(mock_conport)
        
        # Test query processing
        test_queries = [
            "What is the current project status?",
            "What technologies are we using?",
            "What should I work on next?"
        ]
        
        for query in test_queries:
            print(f"\nðŸ“ Testing query: '{query}'")
            
            start_time = datetime.now()
            response = await orchestrator.process_ultimate_request(
                user_query=query,
                workspace_id="/test/workspace"
            )
            end_time = datetime.now()
            
            execution_time = (end_time - start_time).total_seconds()
            
            print(f"  â±ï¸ Execution time: {execution_time:.2f}s")
            print(f"  ðŸŽ¯ Confidence score: {response.confidence_score:.2f}")
            print(f"  ðŸ“Š Response length: {len(response.primary_response)} chars")
            print(f"  ðŸ’¡ Suggestions count: {len(response.proactive_suggestions)}")
            print(f"  ðŸ“ˆ Learning feedback: {response.learning_feedback['status']}")
            
            # Validate response structure
            assert hasattr(response, 'primary_response')
            assert hasattr(response, 'confidence_score')
            assert hasattr(response, 'execution_time')
            assert isinstance(response.proactive_suggestions, list)
            assert isinstance(response.next_actions, list)
        
        print("\nâœ… Orchestrator functionality test completed")
        return True
        
    except Exception as e:
        print(f"âŒ Orchestrator test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    result = asyncio.run(test_orchestrator())
    print(f"Orchestrator test: {'âœ… PASSED' if result else 'âŒ FAILED'}")
```

### 3.2 Individual Engine Tests

```python
# Create test script: test_individual_engines.py
import asyncio

async def test_rag_engine():
    """Test Agentic RAG Engine structure"""
    print("ðŸ§ª Testing Agentic RAG Engine...")
    
    try:
        from rooport.core.rag_engine import (
            AgenticRAGEngine, 
            QueryType, 
            RetrievalStrategy, 
            ConfidenceLevel
        )
        
        # Test enum values
        assert QueryType.SIMPLE == QueryType.SIMPLE
        assert RetrievalStrategy.SEMANTIC_SEARCH == RetrievalStrategy.SEMANTIC_SEARCH
        assert ConfidenceLevel.HIGH == ConfidenceLevel.HIGH
        
        print("âœ… RAG Engine enums and structure working")
        
        # Test initialization (with mock client)
        class MockConPort:
            async def search_decisions_fts(self, **kwargs):
                return []
        
        rag_engine = AgenticRAGEngine(MockConPort())
        print("âœ… RAG Engine initialization successful")
        
        return True
        
    except Exception as e:
        print(f"âŒ RAG Engine test failed: {e}")
        return False

async def test_learning_system():
    """Test Continuous Learning System structure"""
    print("ðŸ§ª Testing Continuous Learning System...")
    
    try:
        from rooport.core.learning_system import (
            ContinuousLearningSystem,
            FeedbackSource,
            FeedbackCategory,
            AgentFeedback
        )
        
        # Test enum values
        assert FeedbackSource.USER_EXPLICIT == FeedbackSource.USER_EXPLICIT
        assert FeedbackCategory.ACCURACY == FeedbackCategory.ACCURACY
        
        print("âœ… Learning System enums and structure working")
        
        # Test feedback data structure
        from datetime import datetime
        feedback = AgentFeedback(
            feedback_id="test-123",
            timestamp=datetime.now(),
            source_of_feedback=FeedbackSource.USER_EXPLICIT,
            agent_slug_target="roo-commander",
            agent_slug_reporter="test-system"
        )
        
        assert feedback.feedback_id == "test-123"
        print("âœ… Learning System data structures working")
        
        return True
        
    except Exception as e:
        print(f"âŒ Learning System test failed: {e}")
        return False

async def test_proactive_engine():
    """Test Proactive Orchestration Engine structure"""
    print("ðŸ§ª Testing Proactive Orchestration Engine...")
    
    try:
        from rooport.core.proactive_engine import ProactiveOrchestrationEngine
        
        # Test initialization
        class MockConPort:
            async def get_progress(self, **kwargs):
                return []
        
        poe = ProactiveOrchestrationEngine(MockConPort())
        print("âœ… Proactive Engine initialization successful")
        
        return True
        
    except Exception as e:
        print(f"âŒ Proactive Engine test failed: {e}")
        return False

async def run_all_engine_tests():
    """Run all individual engine tests"""
    tests = [
        ("RAG Engine", test_rag_engine()),
        ("Learning System", test_learning_system()),
        ("Proactive Engine", test_proactive_engine())
    ]
    
    results = []
    for test_name, test_coro in tests:
        try:
            result = await test_coro
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} test exception: {e}")
            results.append((test_name, False))
    
    print("\nðŸ“Š Individual Engine Test Results:")
    for test_name, passed in results:
        status = "âœ… PASSED" if passed else "âŒ FAILED"
        print(f"  {test_name}: {status}")
    
    all_passed = all(result[1] for result in results)
    return all_passed

if __name__ == "__main__":
    result = asyncio.run(run_all_engine_tests())
    print(f"\nOverall engine tests: {'âœ… ALL PASSED' if result else 'âŒ SOME FAILED'}")
```

---

## ðŸŒ Level 4: Real-World Workflow Testing

### 4.1 Project Initialization Test

```python
# Create test script: test_project_workflow.py
import asyncio
import tempfile
import shutil
from pathlib import Path

async def test_project_initialization():
    """Test complete project initialization workflow"""
    print("ðŸ§ª Testing Project Initialization Workflow...")
    
    # Create temporary project directory
    with tempfile.TemporaryDirectory() as temp_dir:
        project_path = Path(temp_dir) / "test_project"
        project_path.mkdir()
        
        print(f"ðŸ“ Created test project: {project_path}")
        
        # Test 1: Initialize ConPort structure
        context_portal_path = project_path / "context_portal"
        context_portal_path.mkdir()
        
        # Test 2: Create basic project files
        (project_path / "README.md").write_text("# Test Project\n\nROOPORT integration test")
        (project_path / "requirements.txt").write_text("rooport>=1.0.0\n")
        
        # Test 3: Initialize ROOPORT configuration
        config_path = project_path / "config"
        config_path.mkdir()
        
        # Copy configuration template
        mcp_config = {
            "mcpServers": {
                "conport": {
                    "command": "python",
                    "args": ["-m", "context_portal_mcp.main"],
                    "workspace_id": str(project_path)
                }
            }
        }
        
        import json
        (config_path / "mcp.json").write_text(json.dumps(mcp_config, indent=2))
        
        print("âœ… Project structure initialized")
        
        # Test 4: Simulate ROOPORT startup
        try:
            from rooport.config.autonomous_mode import get_autonomous_mode
            enabled, level = get_autonomous_mode()
            print(f"âœ… ROOPORT configuration loaded: {level}")
        except Exception as e:
            print(f"âŒ Configuration loading failed: {e}")
            return False
        
        print("âœ… Project initialization workflow completed")
        return True

async def test_session_workflow():
    """Test complete session workflow"""
    print("ðŸ§ª Testing Session Workflow...")
    
    # Simulate session activities
    session_activities = [
        "Initialize project context",
        "Log initial decisions",
        "Track progress on tasks", 
        "Generate proactive suggestions",
        "Collect performance feedback",
        "Update knowledge graph"
    ]
    
    for activity in session_activities:
        print(f"  ðŸ“ Simulating: {activity}")
        # In real implementation, these would be actual ConPort calls
        await asyncio.sleep(0.1)  # Simulate processing time
    
    print("âœ… Session workflow simulation completed")
    return True

if __name__ == "__main__":
    async def run_workflow_tests():
        test1 = await test_project_initialization()
        test2 = await test_session_workflow()
        
        return test1 and test2
    
    result = asyncio.run(run_workflow_tests())
    print(f"Workflow tests: {'âœ… PASSED' if result else 'âŒ FAILED'}")
```

---

## ðŸ“Š Level 5: Performance and Learning Validation

### 5.1 Performance Benchmarking

```python
# Create test script: test_performance.py
import asyncio
import time
import statistics
from datetime import datetime

async def benchmark_orchestrator_performance():
    """Benchmark orchestrator response times"""
    print("ðŸ§ª Benchmarking Orchestrator Performance...")
    
    # Mock client for consistent testing
    class BenchmarkConPortClient:
        async def log_custom_data(self, **kwargs):
            await asyncio.sleep(0.01)  # Simulate network delay
            return {"status": "success"}
        
        async def get_product_context(self, **kwargs):
            await asyncio.sleep(0.005)
            return {"name": "Benchmark Project"}
    
    try:
        from rooport.core.orchestrator import UltimateAgenticOrchestrator
        
        orchestrator = UltimateAgenticOrchestrator(BenchmarkConPortClient())
        
        # Benchmark different query types
        test_queries = [
            "Simple status query",
            "Complex analysis request with multiple parameters and detailed requirements",
            "What are the current project priorities and next recommended actions?"
        ]
        
        results = {}
        
        for query_type, query in enumerate(test_queries, 1):
            print(f"\nðŸ“Š Benchmarking Query Type {query_type}...")
            
            execution_times = []
            confidence_scores = []
            
            # Run multiple iterations
            for i in range(5):
                start_time = time.time()
                
                response = await orchestrator.process_ultimate_request(
                    user_query=query,
                    workspace_id="/benchmark/workspace"
                )
                
                end_time = time.time()
                execution_time = end_time - start_time
                
                execution_times.append(execution_time)
                confidence_scores.append(response.confidence_score)
                
                print(f"  Iteration {i+1}: {execution_time:.3f}s, confidence: {response.confidence_score:.2f}")
            
            # Calculate statistics
            avg_time = statistics.mean(execution_times)
            std_time = statistics.stdev(execution_times) if len(execution_times) > 1 else 0
            avg_confidence = statistics.mean(confidence_scores)
            
            results[f"Query Type {query_type}"] = {
                "avg_time": avg_time,
                "std_time": std_time,
                "avg_confidence": avg_confidence,
                "min_time": min(execution_times),
                "max_time": max(execution_times)
            }
            
            print(f"  ðŸ“ˆ Average: {avg_time:.3f}s Â± {std_time:.3f}s")
            print(f"  ðŸŽ¯ Avg Confidence: {avg_confidence:.2f}")
        
        # Performance targets
        print("\nðŸŽ¯ Performance Analysis:")
        for query_type, metrics in results.items():
            avg_time = metrics["avg_time"]
            target_met = "âœ…" if avg_time < 3.0 else "âš ï¸" if avg_time < 5.0 else "âŒ"
            print(f"  {query_type}: {avg_time:.3f}s {target_met}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Performance benchmark failed: {e}")
        return False

if __name__ == "__main__":
    result = asyncio.run(benchmark_orchestrator_performance())
    print(f"Performance benchmark: {'âœ… COMPLETED' if result else 'âŒ FAILED'}")
```

---

## ðŸŽ¯ Testing Summary and Validation Matrix

### âœ… What is Currently Functional and Testable:

1. **Core Infrastructure** (100% Functional)
   - Package installation and imports
   - CLI interface and help system
   - Configuration management
   - Autonomous mode control

2. **Orchestrator System** (90% Functional)
   - Query processing pipeline
   - Parallel engine execution
   - Response synthesis
   - Error handling and fallbacks
   - Performance tracking

3. **ConPort Integration** (80% Functional)
   - MCP tool call structure
   - Data persistence concepts
   - Workspace management
   - Configuration templates

### ðŸš§ What Requires Real Data to be Meaningful:

1. **Agentic RAG Engine** (Structure Complete, Needs Data)
   - Query decomposition works
   - Retrieval strategies defined
   - Needs populated ConPort database for meaningful responses

2. **Continuous Learning System** (Framework Ready, Needs Usage)
   - Feedback collection structure complete
   - Performance analysis algorithms ready
   - Needs historical interaction data for learning

3. **Proactive Orchestration** (Logic Ready, Needs Context)
   - Project analysis algorithms complete
   - Suggestion generation framework ready
   - Needs project state data in ConPort for meaningful suggestions

### ðŸ§ª Recommended Testing Progression:

1. **Start with Level 1-2** to verify installation
2. **Progress to Level 3** to test AI engine structure
3. **Use Level 4** to simulate real workflows
4. **Apply Level 5** for performance validation
5. **Real-world usage** with actual projects for full validation

### ðŸ“ˆ Expected Performance Targets:

- **Response Time**: < 3 seconds average
- **Confidence Score**: > 0.7 for most queries
- **Success Rate**: > 95% for basic operations
- **Memory Usage**: < 500MB baseline
- **Startup Time**: < 5 seconds

This testing framework validates both the theoretical capabilities and real-world functionality of ROOPORT, providing clear insights into what works now vs. what needs project data to become meaningful.

"""
Live RAG System Test - Elite Field Service SaaS Platform
Tests the unified RAG system with a live ConPort MCP server.
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
import logging

# Add the rooport module to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'src')))

from rooport.rag.engines.agentic_rag import AgenticRAGEngine, RetrievalStrategy
from rooport.rag.retrievers.conport_retriever import ConPortRetriever
from rooport.rag.retrievers.vector_retriever import VectorRetriever
# Assuming HybridRetriever and InformationSynthesizer are also needed as per original example
from rooport.rag.retrievers.hybrid_retriever import HybridRetriever, FusionStrategy
from rooport.rag.synthesizers.information_synthesizer import InformationSynthesizer, SynthesisStrategy

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')

# This is a placeholder for the AI's ability to use its own MCP tools.
# In a real execution environment, this would be replaced by actual MCP client calls.
# For this test script, we'll need to simulate how the AI would make these calls.
# This is a conceptual challenge for a self-executing test script.
# For now, this will be a mock that *would* use `use_mcp_tool`.

class AiMcpToolUser:
    """
    A conceptual class representing the AI's ability to use its own MCP tools.
    In a real test harness run by an external agent, this would make actual calls.
    For a self-contained Python script, this is tricky.
    We will make it call a *mocked* version of use_mcp_tool for this script to be runnable standalone,
    but with the understanding that the *intent* is to use the live ConPort.
    Alternatively, this script would be run by an agent that *can* make `use_mcp_tool` calls.
    
    For this test, we will assume this class is provided by the testing environment
    and has a method `async def call_mcp_tool(server_name, tool_name, args_dict)`
    """
    async def call_mcp_tool(self, server_name: str, tool_name: str, arguments: Dict[str, Any]) -> Any:
        # This is where the AI would use its <use_mcp_tool>
        # For this script to run standalone for now, we can't actually make that call.
        # This part will need to be adapted if an external agent runs this script.
        logger.info(f"[AiMcpToolUser MOCK] Calling {server_name}.{tool_name} with {arguments}")
        if server_name == "conport":
            if tool_name == "search_decisions_fts":
                # Mock a plausible response structure
                if "architecture" in arguments.get("query_term", "").lower():
                    return {'decisions': [{'id': 101, 'summary': 'Live Test: Microservices decision', 'relevance_score': 0.9}]}
                return {'decisions': []}
            elif tool_name == "search_custom_data_value_fts":
                if "tracking" in arguments.get("query_term", "").lower():
                    return {'results': [{'id': 201, 'category': 'LiveSpecs', 'key': 'real_time', 'value': 'Live GPS data', 'relevance_score': 0.85}]}
                return {'results': []}
            elif tool_name == "semantic_search_conport": # For VectorRetriever
                 if "tracking" in arguments.get("query_text", "").lower():
                    return [{'doc_id': 'vector_doc_1', 'content': 'Live semantic result for tracking', 'score': 0.9, 'metadata': {'source': 'live_vector_db'}}]
                 return []
            # Add other tool mocks as needed by ConPortRetriever
            logger.warning(f"Unmocked ConPort tool in AiMcpToolUser: {tool_name}")
            return {} 
        logger.error(f"Unknown server in AiMcpToolUser mock: {server_name}")
        return None

# Global instance for the client to use
# In a real scenario, this would be how the AI itself makes calls
AI_MCP_CALLER = AiMcpToolUser()

class LiveConPortClient:
    """
    A client that uses the AI's `use_mcp_tool` (via AiMcpToolUser)
    to interact with the live ConPort MCP server.
    Implements methods expected by ConPortRetriever and VectorRetriever (for semantic search).
    """
    def __init__(self, workspace_id: str):
        self.workspace_id = workspace_id
        self.ai_caller = AI_MCP_CALLER # Use the global/provided caller

    async def _call_conport_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        # Ensure workspace_id is always in the arguments for ConPort tools
        full_arguments = {"workspace_id": self.workspace_id, **arguments}
        return await self.ai_caller.call_mcp_tool(server_name="conport", tool_name=tool_name, arguments=full_arguments)

    async def search_decisions_fts(self, workspace_id: str, query_term: str, limit: int = 10):
        # workspace_id is passed for consistency but self.workspace_id is used by _call_conport_tool
        return await self._call_conport_tool("search_decisions_fts", {"query_term": query_term, "limit": limit})

    async def get_system_patterns(self, workspace_id: str, tags_filter_include_all: Optional[List[str]] = None, tags_filter_include_any: Optional[List[str]] = None):
        return await self._call_conport_tool("get_system_patterns", {
            "tags_filter_include_all": tags_filter_include_all,
            "tags_filter_include_any": tags_filter_include_any
        })

    async def search_custom_data_value_fts(self, workspace_id: str, query_term: str, limit: int = 10, category_filter: Optional[str] = None):
        return await self._call_conport_tool("search_custom_data_value_fts", {
            "query_term": query_term, "limit": limit, "category_filter": category_filter
        })

    async def get_progress(self, workspace_id: str, status_filter: Optional[str] = None, parent_id_filter: Optional[int] = None, limit: Optional[int] = None):
        return await self._call_conport_tool("get_progress", {
            "status_filter": status_filter, "parent_id_filter": parent_id_filter, "limit": limit
        })

    async def get_linked_items(self, workspace_id: str, item_type: str, item_id: str, limit: Optional[int] = None):
        return await self._call_conport_tool("get_linked_items", {
            "item_type": item_type, "item_id": item_id, "limit": limit
        })
    
    # Method for VectorRetriever's semantic_search compatibility
    async def semantic_search(self, query: str, max_results: int = 10, similarity_threshold: float = 0.7):
        # This will call ConPort's own semantic search tool
        logger.info(f"LiveConPortClient.semantic_search (via ConPort) for query: {query}")
        # The similarity_threshold is not directly used by ConPort's tool, filtering happens client-side if needed
        results = await self._call_conport_tool("semantic_search_conport", {
            "query_text": query,
            "top_k": max_results
        })
        # Adapt ConPort's semantic search output to what VectorRetriever might expect if necessary
        # Assuming ConPort's tool returns a list of dicts with 'content', 'metadata', 'score'
        return results # Or adapt structure here

    # Mock other methods called by ConPortRetriever if they are not covered by the above
    # For example, retrieve_by_id might call get_decisions, get_system_patterns etc.
    # For simplicity, we assume FTS calls are primary for query-based retrieval.
    async def get_decisions(self, workspace_id: str, limit: Optional[int] = None):
         return await self._call_conport_tool("get_decisions", {"limit": limit})


class LiveSemanticEngineClient:
    """
    Client for the semantic engine, which in this live test,
    will be ConPort's semantic_search_conport tool.
    """
    def __init__(self, workspace_id: str):
        self.workspace_id = workspace_id
        self.ai_caller = AI_MCP_CALLER

    async def semantic_search(self, query: str, max_results: int = 10, similarity_threshold: float = 0.7):
        logger.info(f"LiveSemanticEngineClient.semantic_search (via ConPort) for query: '{query}'")
        # ConPort's semantic_search_conport tool handles top_k.
        # Similarity threshold would typically be applied by the caller if the tool doesn't support it.
        results = await self.ai_caller.call_mcp_tool(
            server_name="conport",
            tool_name="semantic_search_conport",
            arguments={
                "workspace_id": self.workspace_id,
                "query_text": query,
                "top_k": max_results
            }
        )
        # Ensure results are in the format expected by VectorRetriever if different
        # (e.g., list of objects/dicts with 'content', 'metadata', 'score')
        # Assuming ConPort's tool returns a list of dicts like:
        # [{'item_id': 'decision_X', 'item_type': 'decision', 'content_preview': '...', 'score': 0.85, 'metadata': {...}}]
        # We might need to adapt this to a more generic format if VectorRetriever expects something specific.
        # For now, let's assume it's compatible enough or ConPortRetriever handles it.
        
        # Filter by similarity_threshold if the tool doesn't do it
        if results and isinstance(results, list):
            return [r for r in results if r.get('score', 0.0) >= similarity_threshold]
        return []


async def demonstrate_live_rag_system(workspace_id_to_test: str):
    print("üöÄ Elite Field Service - LIVE Unified RAG System Test")
    print("=" * 60)

    # Initialize LIVE clients
    live_conport_client = LiveConPortClient(workspace_id=workspace_id_to_test)
    # The VectorRetriever will use ConPort's semantic search via LiveSemanticEngineClient
    live_semantic_engine_client = LiveSemanticEngineClient(workspace_id=workspace_id_to_test)

    print("\nüì¶ Initializing RAG components with LIVE services...")
    
    conport_retriever = ConPortRetriever(live_conport_client)
    vector_retriever = VectorRetriever(live_semantic_engine_client) # Pass the live client
    hybrid_retriever = HybridRetriever(conport_retriever, vector_retriever)
    
    synthesizer = InformationSynthesizer({'enable_citations': True, 'validation_level': 'moderate'})
    
    rag_engine = AgenticRAGEngine(
        # Note: AgenticRAGEngine in the example took conport_client and semantic_engine separately.
        # We need to ensure its internal logic correctly uses these.
        # For this test, we'll assume it can take our live clients.
        # If AgenticRAGEngine directly instantiates retrievers, we might need to
        # pass the clients to its config or refactor it.
        # For now, let's assume we can pass configured retrievers if needed,
        # or that it uses the passed clients.
        # Based on example, it seems to take clients directly:
        conport_client=live_conport_client, # This client will be used by its internal ConPortRetriever
        semantic_engine=live_semantic_engine_client, # This will be used by its internal VectorRetriever
        config={'max_retrieval_items': 5, 'confidence_threshold': 0.5, 'enable_logging': True}
    )
    
    print("‚úÖ All components initialized successfully (using live client wrappers)!")
    
    test_scenarios = [
        {
            'query': 'What architecture decisions were made for real-time field technician tracking?',
            'description': 'Live Test: Mixed query for decisions and technical details',
            'expected_strategy': RetrievalStrategy.HYBRID # Or whatever the engine decides
        },
        {
            'query': 'Tell me about microservices for the Elite Field Service platform.',
            'description': 'Live Test: ConPort-focused query for project status and decisions',
            'expected_strategy': RetrievalStrategy.CONPORT_ONLY
        },
        {
            'query': 'How is GPS tracking implemented?',
            'description': 'Live Test: Semantic search query for code patterns and implementations',
            'expected_strategy': RetrievalStrategy.SEMANTIC_ONLY
        }
    ]
    
    print(f"\nüß™ Running {len(test_scenarios)} LIVE test scenarios...")
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\n--- Scenario {i}: {scenario['description']} ---")
        print(f"Query: {scenario['query']}")
        
        try:
            start_time = datetime.now()
            result = await rag_engine.process_query(
                query=scenario['query'],
                workspace_id=workspace_id_to_test 
            )
            execution_time = (datetime.now() - start_time).total_seconds()
            
            print(f"\nüìä Results:")
            if result:
                print(f"  ‚Ä¢ Strategy: {result.retrieval_strategy.value if result.retrieval_strategy else 'N/A'}")
                print(f"  ‚Ä¢ Confidence: {result.confidence_level.value if result.confidence_level else 'N/A'} ({result.metadata.get('confidence_score', 0.0):.2f})")
                print(f"  ‚Ä¢ Sources: {len(result.sources_used)} used, {len(result.retrieved_items)} retrieved")
                print(f"  ‚Ä¢ Execution time: {execution_time:.2f}s")
                print(f"\nüí¨ Answer:")
                print(f"  {result.final_answer}")
                if result.reasoning_chain:
                    print(f"\nüß† Reasoning:")
                    for step in result.reasoning_chain[:3]: print(f"  ‚Ä¢ {step}")
            else:
                print("  ‚ö†Ô∏è No result object returned from RAG engine.")

        except Exception as e:
            logger.error(f"‚ùå Error in live scenario {i} ('{scenario['query']}'): {e}", exc_info=True)
            print(f"‚ùå Error: {e}")
            continue
            
    print(f"\nüéâ Live RAG demo completed!")
    # We can't easily get metrics from the *actual* ConPort server here,
    # but the AiMcpToolUser mock could be enhanced to track calls if needed for pure script testing.
    # The RAG engine's own metrics would reflect calls to our live client wrappers.
    rag_metrics = rag_engine.get_performance_metrics()
    print(f"\nRAG Engine Metrics (based on calls to live client wrappers):")
    print(f"  ‚Ä¢ Total queries: {rag_metrics['total_queries']}")
    print(f"  ‚Ä¢ Success rate: {rag_metrics['success_rate']:.1%}") # Might be 0 if errors above
    print(f"  ‚Ä¢ Average response time: {rag_metrics['avg_response_time']:.2f}s")

async def main():
    workspace = "c:/Users/thegr/OneDrive/Desktop/frontendtest/integration" # Use the actual workspace ID
    # Ensure ConPort server is running and accessible by the AI executing this.
    # Ensure GOOGLE_APPLICATION_CREDENTIALS is set if VertexAI is used by synthesizer (not in this example's synthesizer)
    
    # For this script to be *truly* testable standalone without an AI running it,
    # AiMcpToolUser would need to be a real MCP client.
    # Since an AI *is* running it, the AI_MCP_CALLER will use the AI's actual use_mcp_tool.
    # The mock calls within AiMcpToolUser are just for illustration if run totally isolated.
    # Let's assume the AI running this will replace `AI_MCP_CALLER.call_mcp_tool` with its actual MCP call.
    # To make this clearer, we can make AiMcpToolUser abstract or raise NotImplementedError.
    # For now, the above mock in AiMcpToolUser will be hit if this script is run `python test_live_rag.py`
    # but the *intent* is for the AI to use its live MCP calls.
    
    # To make it use the AI's tools, the AI would effectively replace the body of
    # AiMcpToolUser.call_mcp_tool with its own <use_mcp_tool> logic.
    # This script is more of a *template* for the AI to execute.
    
    print("This script is a template for an AI to execute using its live MCP capabilities.")
    print("It defines a LiveConPortClient that, when used by the AI, will make real calls to the 'conport' MCP server.")
    print("If you are an AI, please adapt the AiMcpToolUser.call_mcp_tool method or replace its calls.")
    print(f"Attempting to run demonstration for workspace: {workspace}\n")

    # This is the part the AI should effectively execute:
    await demonstrate_live_rag_system(workspace_id_to_test=workspace)

if __name__ == "__main__":
    # This setup allows the script to be run, but it will use the MOCK AiMcpToolUser.
    # An AI executing this task would use its actual MCP tool capabilities.
    asyncio.run(main())